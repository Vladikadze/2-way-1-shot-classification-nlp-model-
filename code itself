import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer

# =======================
# Config
# =======================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
BATCH_SIZE = 2
EPOCHS = 10
LR = 1e-4

text_1 = """ Paper 1 """ # removed for privacy reasns 
text_2 = """ Paper 2 """
text_6 = """ Paper 6 """
text_7 = """ Paper 7 """

# =======================
# Fine Texts + Labels (Full Text)
# =======================
fine_data = [
    # Paper 1
    (text_1, 0),

    # Paper 6
    (text_6, 0),

    # Paper 7 (with matching appeal)
    (text_7, 1),
]

# =======================
# Dataset
# =======================
class FineDataset(Dataset):
    def __init__(self, data, encoder):
        self.samples = []
        for text, label in data:
            with torch.no_grad():
                embedding = encoder.encode(text, convert_to_tensor=True, device=DEVICE)
            self.samples.append((embedding, torch.tensor(label, dtype=torch.float)))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]

# =======================
# Classifier Model
# =======================
class FineClassifier(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return torch.sigmoid(self.fc(x))

# =======================
# Training Function
# =======================
def train(model, dataloader, optimizer, criterion):
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for X, y in dataloader:
            y_pred = model(X)
            loss = criterion(y_pred.view(-1), y.to(DEVICE).view(-1))  # Fix shape mismatch
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}")

# =======================
# Inference Function
# =======================
def predict(model, encoder, text):
    with torch.no_grad():
        embedding = encoder.encode(text, convert_to_tensor=True, device=DEVICE)
        prob = model(embedding.unsqueeze(0)).item()
        return prob

# =======================
# Run
# =======================
if __name__ == "__main__":
    encoder = SentenceTransformer(EMBEDDING_MODEL).to(DEVICE)

    dataset = FineDataset(fine_data, encoder)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    input_dim = dataset[0][0].shape[0]
    model = FineClassifier(input_dim).to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)
    criterion = nn.BCELoss()

    train(model, dataloader, optimizer, criterion)

    # Try on a new fine
    new_fine = (text_2)

    score = predict(model, encoder, new_fine)
    print(f"\nAppeal likelihood: {score:.4f}")


    if score > 0.5:
        print("The fine is likely to be appealed.")
    else:
        print("The fine is unlikely to be appealed.")
        
